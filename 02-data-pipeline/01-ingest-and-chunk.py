# Databricks notebook source
# ========================================
# ãƒ‡ãƒ¼ã‚¿å–å¾— + ãƒãƒ£ãƒ³ã‚¯åŒ–ï¼ˆçµ±åˆç‰ˆï¼‰
# ========================================

# COMMAND ----------

%pip install transformers langchain>=0.3.0 langchain-core>=0.3.0 langchain-community lxml beautifulsoup4 requests --quiet
dbutils.library.restartPython()

# COMMAND ----------

%run ../00-config

# COMMAND ----------

# ãƒ‡ãƒ¼ã‚¿å–å¾—
print("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")

sample_docs = [
    {"url": f"https://example.com/doc{i}", "text": f"ã‚µãƒ³ãƒ—ãƒ«æ–‡æ›¸{i}ã®å†…å®¹ã§ã™ã€‚Databricksã¯ãƒ‡ãƒ¼ã‚¿ãƒ¬ã‚¤ã‚¯ãƒã‚¦ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã€Apache Sparkã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ã¦ã„ã¾ã™ã€‚" * 30}
    for i in range(1, 6)
]

raw_df = spark.createDataFrame(sample_docs)

# å†ªç­‰æ€§: æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä¸Šæ›¸ã
raw_df.write.mode("overwrite").saveAsTable(RAW_DOCS_TABLE)
print(f"âœ… {raw_df.count()}ä»¶ã‚’ {RAW_DOCS_TABLE} ã«ä¿å­˜")

# COMMAND ----------

# ãƒãƒ£ãƒ³ã‚¯åŒ–
print("\nâœ‚ï¸  ãƒãƒ£ãƒ³ã‚¯åŒ–ä¸­...")

from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import OpenAIGPTTokenizer
import pyspark.sql.functions as F
from pyspark.sql.functions import pandas_udf
import pandas as pd

tokenizer = OpenAIGPTTokenizer.from_pretrained("openai-gpt")
text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(
    tokenizer,
    chunk_size=CHUNK_CONFIG["max_size"],
    chunk_overlap=CHUNK_CONFIG["overlap"]
)

@pandas_udf("array<string>")
def chunk_text(texts: pd.Series) -> pd.Series:
    def split(text):
        if not text or len(text.strip()) < CHUNK_CONFIG["min_size"]:
            return []
        chunks = text_splitter.split_text(text)
        return [c for c in chunks if len(tokenizer.encode(c)) >= CHUNK_CONFIG["min_size"]]
    return texts.apply(split)

# COMMAND ----------

# ãƒãƒ£ãƒ³ã‚¯åŒ–å®Ÿè¡Œ
df = spark.table(RAW_DOCS_TABLE)

chunked_df = (df
    .withColumn("chunks", chunk_text(F.col("text")))
    .withColumn("content", F.explode(F.col("chunks")))
    .withColumn("id", F.monotonically_increasing_id())
    .select("id", "url", "content")
)

# Change Data Feedæœ‰åŠ¹åŒ–
(chunked_df
    .write
    .mode("overwrite")
    .option("delta.enableChangeDataFeed", "true")
    .saveAsTable(CHUNKED_DOCS_TABLE))

print(f"âœ… {chunked_df.count()}ãƒãƒ£ãƒ³ã‚¯ã‚’ {CHUNKED_DOCS_TABLE} ã«ä¿å­˜")

# COMMAND ----------

# çµæœç¢ºèª
display(spark.table(CHUNKED_DOCS_TABLE).limit(5))

# COMMAND ----------

# ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
print("\nğŸ“Š ãƒãƒ£ãƒ³ã‚¯ã‚µãƒ³ãƒ—ãƒ«:")
for row in spark.table(CHUNKED_DOCS_TABLE).limit(3).collect():
    print(f"\nID: {row['id']}")
    print(f"URL: {row['url']}")
    print(f"Content: {row['content'][:100]}...")