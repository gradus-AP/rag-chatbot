# Databricks notebook source
# ========================================
# RAGãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰
# ========================================

# COMMAND ----------

%pip install --upgrade databricks-langchain langchain-community langchain>=0.3.0 langchain-core>=0.3.0 databricks-sql-connector databricks-vectorsearch mlflow --quiet
dbutils.library.restartPython()

# COMMAND ----------

%run ../00-config

# COMMAND ----------

import os
from langchain.chains import RetrievalQA
from langchain_core.prompts import PromptTemplate
from langchain_community.chat_models import ChatDatabricks
from langchain_community.vectorstores import DatabricksVectorSearch
from langchain_community.embeddings import DatabricksEmbeddings
from databricks.vector_search.client import VectorSearchClient

# èªè¨¼è¨­å®š
os.environ['DATABRICKS_TOKEN'] = dbutils.secrets.get(SECRET_SCOPE, SECRET_KEY)
os.environ["DATABRICKS_HOST"] = HOST

print("âœ… ç’°å¢ƒå¤‰æ•°è¨­å®šå®Œäº†")

# COMMAND ----------

# Retrieveræ§‹ç¯‰
def get_retriever():
    '''Vector Searchãƒ™ãƒ¼ã‚¹ã®retrieverã‚’è¿”ã™'''
    vsc = VectorSearchClient(
        workspace_url=HOST,
        personal_access_token=os.environ["DATABRICKS_TOKEN"]
    )
    vs_index = vsc.get_index(
        endpoint_name=VECTOR_SEARCH_ENDPOINT,
        index_name=VECTOR_INDEX_NAME
    )
    embedding = DatabricksEmbeddings(endpoint=EMBEDDING_MODEL_ENDPOINT)
    vectorstore = DatabricksVectorSearch(
        vs_index,
        text_column="content",
        embedding=embedding
    )
    return vectorstore.as_retriever()

print("âœ… Retrieveré–¢æ•°å®šç¾©å®Œäº†")

# COMMAND ----------

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
TEMPLATE = """ä»¥ä¸‹ã®æƒ…å ±ã‚’ä½¿ã£ã¦è³ªå•ã«æ—¥æœ¬èªã§ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚
ç­”ãˆãŒã‚ã‹ã‚‰ãªã„å ´åˆã¯ã€Œã‚ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚

æƒ…å ±:
{context}

è³ªå•: {question}

å›ç­”:"""

prompt = PromptTemplate(template=TEMPLATE, input_variables=["context", "question"])

print("âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½œæˆå®Œäº†")

# COMMAND ----------

# LLMãƒ¢ãƒ‡ãƒ«
chat_model = ChatDatabricks(endpoint=LLM_ENDPOINT, max_tokens=500)

print(f"âœ… LLMãƒ¢ãƒ‡ãƒ«æ¥ç¶šå®Œäº†: {LLM_ENDPOINT}")

# COMMAND ----------

# RAGãƒã‚§ãƒ¼ãƒ³ä½œæˆ
rag_chain = RetrievalQA.from_chain_type(
    llm=chat_model,
    chain_type="stuff",
    retriever=get_retriever(),
    chain_type_kwargs={"prompt": prompt}
)

print("âœ… RAGãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰å®Œäº†")

# COMMAND ----------

# ç°¡æ˜“ãƒ†ã‚¹ãƒˆ
print("\nğŸ§ª ç°¡æ˜“å‹•ä½œç¢ºèª...")

test_questions = [
    "Databricksã¨ã¯ï¼Ÿ",
    "ã‚µãƒ³ãƒ—ãƒ«æ–‡æ›¸ã«ã¤ã„ã¦æ•™ãˆã¦"
]

for q in test_questions:
    print(f"\nè³ªå•: {q}")
    result = rag_chain.run({"query": q})
    print(f"å›ç­”: {result}")
    print("-" * 60)

print("\nâœ… RAGãƒã‚§ãƒ¼ãƒ³å‹•ä½œç¢ºèªå®Œäº†")