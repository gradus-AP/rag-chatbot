# Databricks notebook source
# ========================================
# ãƒ†ã‚¹ãƒˆ + MLflowç™»éŒ²
# ========================================

# COMMAND ----------

%run ./01-build-rag-chain

# COMMAND ----------

# ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹å®Ÿè¡Œ
test_cases = [
    {
        "query": "Databricksã¨ã¯ï¼Ÿ",
        "expected_keywords": ["Databricks", "ãƒ‡ãƒ¼ã‚¿", "ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ "]
    },
    {
        "query": "Sparkã«ã¤ã„ã¦æ•™ãˆã¦",
        "expected_keywords": ["Spark", "Apache"]
    },
    {
        "query": "ã‚µãƒ³ãƒ—ãƒ«æ–‡æ›¸ã®å†…å®¹ã¯ï¼Ÿ",
        "expected_keywords": ["ã‚µãƒ³ãƒ—ãƒ«", "æ–‡æ›¸"]
    }
]

print("ğŸ§ª ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...\n")
print("="*60)

results = []

for i, test in enumerate(test_cases, 1):
    print(f"\nTest {i}: {test['query']}")

    try:
        answer = ask_question(test["query"])
        print(f"å›ç­”: {answer}")

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
        found = [kw for kw in test["expected_keywords"] if kw in answer]

        if found:
            print(f"âœ… PASS (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(found)})")
            results.append(True)
        else:
            print(f"âš ï¸  WARNING (æœŸå¾…ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãªã—: {', '.join(test['expected_keywords'])})")
            results.append(False)

    except Exception as e:
        print(f"âŒ ERROR: {e}")
        results.append(False)

    print("-" * 60)

passed = sum(results)
print(f"\nçµæœ: {passed}/{len(test_cases)} passed")

# COMMAND ----------

# MLflowç™»éŒ²
import mlflow
from mlflow.models import infer_signature
from mlflow.pyfunc import PythonModel

mlflow.set_registry_uri("databricks-uc")

print(f"\nğŸ“¦ MLflowã«ãƒ¢ãƒ‡ãƒ«ç™»éŒ²ä¸­...")
print(f"   ãƒ¢ãƒ‡ãƒ«å: {MODEL_NAME}")

# COMMAND ----------

# ã‚«ã‚¹ã‚¿ãƒ PyFuncãƒ¢ãƒ‡ãƒ«ã§ãƒ©ãƒƒãƒ—
class RAGChatbot(PythonModel):
    """RAG Chatbot MLflow wrapper"""

    def load_context(self, context):
        """ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰"""
        import os
        from databricks.vector_search.client import VectorSearchClient
        from databricks_langchain import ChatDatabricks

        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿
        self.vector_search_endpoint = os.environ.get('VECTOR_SEARCH_ENDPOINT')
        self.vector_index_name = os.environ.get('VECTOR_INDEX_NAME')
        self.llm_endpoint = os.environ.get('LLM_ENDPOINT')
        self.host = os.environ.get('DATABRICKS_HOST')
        self.token = os.environ.get('DATABRICKS_TOKEN')

        # Vector Searchæ¥ç¶š
        self.vsc = VectorSearchClient(
            workspace_url=self.host,
            personal_access_token=self.token
        )
        self.vs_index = self.vsc.get_index(
            endpoint_name=self.vector_search_endpoint,
            index_name=self.vector_index_name
        )

        # LLMåˆæœŸåŒ–
        self.llm = ChatDatabricks(
            endpoint=self.llm_endpoint,
            temperature=0.1,
            max_tokens=500
        )

    def predict(self, context, model_input):
        """è³ªå•ã«å¯¾ã—ã¦å›ç­”ã‚’ç”Ÿæˆ"""
        # model_inputã¯DataFrameã¾ãŸã¯dict
        if hasattr(model_input, 'to_dict'):
            questions = model_input.to_dict('records')
        else:
            questions = [model_input] if isinstance(model_input, dict) else model_input

        answers = []
        for item in questions:
            question = item.get('query') or item.get('question')

            # Vector Search
            search_results = self.vs_index.similarity_search(
                query_text=question,
                columns=["content", "url"],
                num_results=3
            )

            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
            documents = search_results.get('result', {}).get('data_array', [])
            if not documents:
                answers.append("é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                continue

            context_text = "\n\n".join([doc[0] for doc in documents])

            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
            prompt = f"""ä»¥ä¸‹ã®æƒ…å ±ã‚’ä½¿ã£ã¦è³ªå•ã«æ—¥æœ¬èªã§ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚
ç­”ãˆãŒã‚ã‹ã‚‰ãªã„å ´åˆã¯ã€Œã‚ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚

æƒ…å ±:
{context_text}

è³ªå•: {question}

å›ç­”:"""

            # LLMå‘¼ã³å‡ºã—
            response = self.llm.invoke(prompt)
            answer = response.content if hasattr(response, 'content') else str(response)
            answers.append(answer)

        return answers

# COMMAND ----------

# ã‚·ã‚°ãƒãƒãƒ£ä½œæˆ
question_input = {"query": "Databricksã¨ã¯ï¼Ÿ"}
test_answer = ask_question(question_input["query"])
signature = infer_signature(question_input, test_answer)

# COMMAND ----------

# ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šï¼ˆãƒ¢ãƒ‡ãƒ«ãŒä½¿ç”¨ï¼‰
import os
os.environ['VECTOR_SEARCH_ENDPOINT'] = VECTOR_SEARCH_ENDPOINT
os.environ['VECTOR_INDEX_NAME'] = VECTOR_INDEX_NAME
os.environ['LLM_ENDPOINT'] = LLM_ENDPOINT

# ãƒ¢ãƒ‡ãƒ«ç™»éŒ²
with mlflow.start_run(run_name=f"rag_chatbot_{ENV}") as run:
    model_info = mlflow.pyfunc.log_model(
        artifact_path="model",
        python_model=RAGChatbot(),
        registered_model_name=MODEL_NAME,
        pip_requirements=[
            "mlflow",
            "databricks-vectorsearch",
            "databricks-langchain"
        ],
        input_example=question_input,
        signature=signature
    )

    print(f"âœ… ãƒ¢ãƒ‡ãƒ«ç™»éŒ²å®Œäº†: {MODEL_NAME}")
    print(f"   Run ID: {run.info.run_id}")
    print(f"   Version: {model_info.registered_model_version}")

# COMMAND ----------

displayHTML(f"""
<div style="padding: 20px; background-color: #e8f5e9; border-radius: 10px;">
<h2>âœ… ãƒ¢ãƒ‡ãƒ«ç™»éŒ²å®Œäº†ï¼</h2>
<p><strong>ãƒ¢ãƒ‡ãƒ«å:</strong> {MODEL_NAME}</p>
<p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³:</strong> {model_info.registered_model_version}</p>
<p><a href="#mlflow/models/{MODEL_NAME}" target="_blank">ğŸ“Š ãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèª</a></p>
<p>æ¬¡ã¯ <code>03-model/03-deploy</code> ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„</p>
</div>
""")
